{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FootballLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(FootballLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM for general features\n",
    "        self.lstm_general = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Separate LSTM for h2h features\n",
    "        self.lstm_h2h = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Output layers for multi-output predictions\n",
    "        self.fc_result = nn.Linear(hidden_dim * 2, 3)  # match result (win/lose/draw)\n",
    "        self.fc_goals = nn.Linear(hidden_dim * 2, 1)  # number of goals\n",
    "        self.fc_corners = nn.Linear(hidden_dim * 2, 1)  # number of corner kicks\n",
    "        self.fc_fouls = nn.Linear(hidden_dim * 2, 1)  # number of fouls\n",
    "        self.fc_cards = nn.Linear(hidden_dim * 2, 1)  # number of cards\n",
    "\n",
    "    def forward(self, general_features, h2h_features):\n",
    "        # Processing general features\n",
    "        _, (ht_general, _) = self.lstm_general(general_features)  # ht_general shape: [num_layers, batch, hidden_dim]\n",
    "        \n",
    "        # Processing head-to-head features\n",
    "        _, (ht_h2h, _) = self.lstm_h2h(h2h_features)  # ht_h2h shape: [num_layers, batch, hidden_dim]\n",
    "        \n",
    "        # Concatenating the final hidden states from the last layer of both LSTMs\n",
    "        # Take the last layer's hidden states (last layer from both LSTMs)\n",
    "        ht_general_last = ht_general[-1]  # Last layer, shape: [batch, hidden_dim]\n",
    "        ht_h2h_last = ht_h2h[-1]  # Last layer, shape: [batch, hidden_dim]\n",
    "        \n",
    "        # Concatenate along the feature dimension\n",
    "        combined_ht = torch.cat((ht_general_last, ht_h2h_last), dim=1)  # shape: [batch, 2 * hidden_dim]\n",
    "        \n",
    "        # Output calculations\n",
    "        result = self.fc_result(combined_ht)  # Predicted match result\n",
    "        goals = self.fc_goals(combined_ht)  # Predicted number of goals\n",
    "        corners = self.fc_corners(combined_ht)  # Predicted number of corner kicks\n",
    "        fouls = self.fc_fouls(combined_ht)  # Predicted number of fouls\n",
    "        cards = self.fc_cards(combined_ht)  # Predicted number of cards\n",
    "        \n",
    "        return result, goals, corners, fouls, cards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykładowe dane\n",
    "batch_size = 1\n",
    "seq_length = 5\n",
    "input_dim = 10\n",
    "\n",
    "general_features = torch.randn(batch_size, seq_length, input_dim)\n",
    "h2h_features = torch.randn(batch_size, seq_length, input_dim)\n",
    "\n",
    "# Inicjalizacja modelu\n",
    "model = FootballLSTM(input_dim=input_dim, hidden_dim=100, num_layers=2)\n",
    "\n",
    "# Uzyskanie przewidywań\n",
    "result, goals, corners, fouls, cards = model(general_features, h2h_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0461, -0.0845, -0.0427]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[0.0199]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.0095]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.0481]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[0.0009]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(general_features, h2h_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prawdopodobieństwa wyniku meczu: tensor([[0.3581, 0.3142, 0.3277]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "result_probabilities = F.softmax(result, dim=1)\n",
    "print(\"Prawdopodobieństwa wyniku meczu:\", result_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.3581 + 0.3142 + 0.3277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana liczba goli: 0\n",
      "Przewidywana liczba rzutów rożnych: 0\n",
      "Przewidywana liczba fauli: 0\n",
      "Przewidywana liczba kartek: 0\n"
     ]
    }
   ],
   "source": [
    "predicted_goals = round(goals.item())\n",
    "predicted_corners = round(corners.item())\n",
    "predicted_fouls = round(fouls.item())\n",
    "predicted_cards = round(cards.item())\n",
    "\n",
    "print(f\"Przewidywana liczba goli: {predicted_goals}\")\n",
    "print(f\"Przewidywana liczba rzutów rożnych: {predicted_corners}\")\n",
    "print(f\"Przewidywana liczba fauli: {predicted_fouls}\")\n",
    "print(f\"Przewidywana liczba kartek: {predicted_cards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0199]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykładowe dane\n",
    "general_data = pd.DataFrame(np.random.rand(10, 5))  # 10 próbek, 5 cech\n",
    "h2h_data = pd.DataFrame(np.random.rand(10, 5))  # 10 próbek, 5 cech\n",
    "\n",
    "# Konwersja na Tensory PyTorch\n",
    "general_features = torch.tensor(general_data.values, dtype=torch.float32)\n",
    "h2h_features = torch.tensor(h2h_data.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8186, 0.6336, 0.2327, 0.9308, 0.4397],\n",
       "        [0.5828, 0.7696, 0.5506, 0.0372, 0.1350],\n",
       "        [0.1965, 0.4906, 0.2309, 0.4378, 0.1039],\n",
       "        [0.9084, 0.5413, 0.7994, 0.7978, 0.1618],\n",
       "        [0.2549, 0.5177, 0.3100, 0.5998, 0.5813],\n",
       "        [0.5358, 0.3724, 0.5937, 0.7571, 0.9037],\n",
       "        [0.3314, 0.1399, 0.2412, 0.2985, 0.4733],\n",
       "        [0.0912, 0.2866, 0.0975, 0.5327, 0.4665],\n",
       "        [0.5683, 0.0579, 0.6679, 0.9305, 0.0301],\n",
       "        [0.6243, 0.7807, 0.9294, 0.0055, 0.3299]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Użycie modelu\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m FootballLSTM(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m result, goals, corners, fouls, cards \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneral_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh2h_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\magisterka_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\magisterka_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 36\u001b[0m, in \u001b[0;36mFootballLSTM.forward\u001b[1;34m(self, general_features, h2h_features)\u001b[0m\n\u001b[0;32m     33\u001b[0m ht_h2h_last \u001b[38;5;241m=\u001b[39m ht_h2h[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Last layer, shape: [batch, hidden_dim]\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Concatenate along the feature dimension\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m combined_ht \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mht_general_last\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mht_h2h_last\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: [batch, 2 * hidden_dim]\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Output calculations\u001b[39;00m\n\u001b[0;32m     39\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_result(combined_ht)  \u001b[38;5;66;03m# Predicted match result\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# Załóżmy, że mamy DataFrame'y Pandas dla obu rodzajów danych\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Przykładowe dane\n",
    "general_data = pd.DataFrame(np.random.rand(10, 5))  # 10 próbek, 5 cech\n",
    "h2h_data = pd.DataFrame(np.random.rand(10, 5))  # 10 próbek, 5 cech\n",
    "\n",
    "# Konwersja na Tensory PyTorch\n",
    "general_features = torch.tensor(general_data.values, dtype=torch.float32)\n",
    "h2h_features = torch.tensor(h2h_data.values, dtype=torch.float32)\n",
    "\n",
    "# Użycie modelu\n",
    "model = FootballLSTM(input_dim=5, hidden_dim=100, num_layers=2)\n",
    "result, goals, corners, fouls, cards = model(general_features, h2h_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.LSTM(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.randn(1,3) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-2.4757, -0.0613,  0.8765]]),\n",
       " tensor([[-0.7576,  0.5321,  1.3035]]),\n",
       " tensor([[ 1.1223, -0.4409,  0.8695]]),\n",
       " tensor([[-0.7415,  0.7655,  0.2143]]),\n",
       " tensor([[0.6256, 0.0883, 0.9677]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state = (torch.randn(1,1,3), torch.randn(1,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inputs:\n",
    "    out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0234, -0.4240,  0.0158]]], grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "(tensor([[[-0.0234, -0.4240,  0.0158]]], grad_fn=<StackBackward0>), tensor([[[-0.1086, -0.6122,  0.0251]]], grad_fn=<StackBackward0>))\n",
      "tensor([[[-0.0343, -0.4233, -0.1624]]], grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "(tensor([[[-0.0343, -0.4233, -0.1624]]], grad_fn=<StackBackward0>), tensor([[[-0.1067, -0.6368, -0.3032]]], grad_fn=<StackBackward0>))\n",
      "tensor([[[-0.1239, -0.2604, -0.1551]]], grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "(tensor([[[-0.1239, -0.2604, -0.1551]]], grad_fn=<StackBackward0>), tensor([[[-0.3531, -0.6143, -0.5484]]], grad_fn=<StackBackward0>))\n",
      "tensor([[[-0.0354, -0.3647, -0.0885]]], grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "(tensor([[[-0.0354, -0.3647, -0.0885]]], grad_fn=<StackBackward0>), tensor([[[-0.1265, -0.5377, -0.1604]]], grad_fn=<StackBackward0>))\n",
      "tensor([[[-0.0843, -0.3082, -0.1784]]], grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "(tensor([[[-0.0843, -0.3082, -0.1784]]], grad_fn=<StackBackward0>), tensor([[[-0.2376, -0.5770, -0.4886]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "for i in inputs:\n",
    "    out, hidden = model(i.view(1,1,-1), hidden_state)\n",
    "    print(out)\n",
    "    print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4757, -0.0613,  0.8765]],\n",
       "\n",
       "        [[-0.7576,  0.5321,  1.3035]],\n",
       "\n",
       "        [[ 1.1223, -0.4409,  0.8695]],\n",
       "\n",
       "        [[-0.7415,  0.7655,  0.2143]],\n",
       "\n",
       "        [[ 0.6256,  0.0883,  0.9677]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = model(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0027,  0.1892,  0.0612]],\n",
      "\n",
      "        [[-0.0150, -0.0627, -0.1976]],\n",
      "\n",
      "        [[-0.1869, -0.0908, -0.1664]],\n",
      "\n",
      "        [[-0.0975, -0.1816, -0.3243]],\n",
      "\n",
      "        [[-0.1468, -0.1239, -0.2429]]], grad_fn=<MkldnnRnnLayerBackward0>)\n",
      "(tensor([[[-0.1468, -0.1239, -0.2429]]], grad_fn=<StackBackward0>), tensor([[[-0.2601, -0.2058, -0.6776]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1    # The number of variables in your sequence data. \n",
    "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
    "n_layers   = 2    # The total number of LSTM models layers\n",
    "out_size   = 1    # The size of the output you desire from your RNN\n",
    "\n",
    "lstm   = torch.nn.LSTM(input_size, n_hidden, n_layers)\n",
    "linear = torch.nn.Linear(n_hidden, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_batches\u001b[49m(data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_batches' is not defined"
     ]
    }
   ],
   "source": [
    "get_batches(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Przykład szeregu czasowego\n",
    "data = np.random.randn(1000)  # 1000 punktów danych\n",
    "\n",
    "# Definicja długości sekwencji\n",
    "seq_length = 10\n",
    "\n",
    "# Przygotowanie danych do LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for i in range(len(data)-seq_length):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        y = data[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Tworzenie sekwencji\n",
    "x_data, y_data = create_sequences(data, seq_length)\n",
    "\n",
    "# Konwersja na tensor PyTorch\n",
    "x_data = torch.tensor(x_data, dtype=torch.float32).view(-1, seq_length, 1)  # Reshape to (batch_size, seq_length, num_features)\n",
    "y_data = torch.tensor(y_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    n_batches = len(x) // batch_size\n",
    "    x = x[:n_batches * batch_size]\n",
    "    y = y[:n_batches * batch_size]\n",
    "    \n",
    "    for i in range(0, len(x), batch_size):\n",
    "        yield x[i:i+batch_size], y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size):\n",
    "    \"\"\"\n",
    "    Dzieli dane na partie o określonym rozmiarze.\n",
    "    :param data: Tensor danych o wymiarach (total_samples, seq_length, num_features)\n",
    "    :param batch_size: Rozmiar jednej partii danych\n",
    "    :return: Generator partii danych\n",
    "    \"\"\"\n",
    "    total_samples = data.size(0)\n",
    "    \n",
    "    # Przechodzimy przez dane w krokach o wielkości `batch_size`\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        x = data[i:i + batch_size]\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_batches at 0x000001DD92659540>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batches(data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3402, -1.0346, -1.6351,  0.5458,  1.9475,  0.3342, -1.0398,\n",
       "          -0.6201,  0.6268,  0.5607],\n",
       "         [ 0.7123, -1.9538, -0.5583, -0.7741,  1.1485,  0.7548, -0.7387,\n",
       "           1.1059, -2.3533, -0.1311],\n",
       "         [-1.3142, -0.7927,  0.6968,  1.5350, -0.0881,  0.0739,  1.1037,\n",
       "           1.8096, -0.2372, -2.1510]],\n",
       "\n",
       "        [[-1.7891, -1.1571, -0.3987, -0.4641, -0.2216, -1.7661,  0.1770,\n",
       "           0.3268, -1.0840, -2.1622],\n",
       "         [ 0.4804, -0.6694,  0.8168,  0.0583,  0.1769, -0.7708,  1.2248,\n",
       "          -1.1555, -0.9373,  0.1350],\n",
       "         [ 0.9253,  0.7556, -0.9968,  1.2333,  0.8377, -0.1685,  3.7096,\n",
       "          -1.1110, -1.3701, -0.0440]],\n",
       "\n",
       "        [[ 1.8915, -0.4202,  0.8943,  0.9341, -0.2192,  0.2484, -2.0439,\n",
       "           0.3170, -0.5367, -0.0198],\n",
       "         [ 0.9351,  0.0613,  1.0629,  0.3688,  0.3416, -1.2335, -0.3948,\n",
       "          -2.3961, -0.1503, -0.2130],\n",
       "         [ 0.4049,  1.4194,  0.4130, -1.0066, -0.6485,  1.3802, -1.7308,\n",
       "          -0.6369,  0.0708, -1.7277]],\n",
       "\n",
       "        [[ 0.6262,  1.7703, -0.5694, -1.8825, -0.1068, -1.2851, -0.6668,\n",
       "           0.6638,  1.4076, -1.0543],\n",
       "         [ 0.8792, -0.5519,  1.2368, -0.2810, -0.5305, -0.4284,  0.6808,\n",
       "          -0.8382, -0.4026,  0.5003],\n",
       "         [-0.0345,  0.3043,  0.5721,  0.6013, -0.3179,  0.1337, -1.9518,\n",
       "          -1.2101,  1.8731, -1.0583]],\n",
       "\n",
       "        [[ 2.4174, -1.3561, -1.0687, -0.7233, -0.6632,  0.5574, -0.3838,\n",
       "          -1.1651,  0.2408,  0.0545],\n",
       "         [ 0.8874,  0.7671,  0.2272,  0.5832, -0.9094,  0.5900,  1.0694,\n",
       "          -0.2390,  2.5604, -0.8547],\n",
       "         [ 0.6808,  0.6542, -1.2112, -0.3541, -1.1204, -0.0040, -1.9726,\n",
       "           1.1431, -0.0334, -0.6708]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magisterka_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
